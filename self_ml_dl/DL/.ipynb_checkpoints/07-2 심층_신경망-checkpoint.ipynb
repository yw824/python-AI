{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMvTmPK6ulu_"
   },
   "source": [
    "7-1에서 했던 인공 신경망의 성능을 더 높여 보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e8UAeCwu0zY"
   },
   "source": [
    "## 2개의 층 \n",
    "- 다시 케라스 API를 사용해 패션 MNIST 데이터셋을 불러오자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eVX99QgjvGc5"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rO7hsytmvSMS"
   },
   "source": [
    "그 다음 이미지의 픽셀값을 0 ~ 255 범위에서 0 ~ 1 사이로 변환하고, <br>\n",
    "28 * 28 크기의 2차원 배열을 784 크기의 1차원 배열로 변환하자. <br>\n",
    "마지막으로 train_test_split() 함수로 훈련 세트와 검증 세트로 나눈다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bGdWwLARvmSh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_scaled = train_input / 255.0\n",
    "train_scaled = train_scaled.reshape(-1, 28*28)\n",
    "\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(\n",
    "    train_scaled, train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD7BFxjTwBIQ"
   },
   "source": [
    "이제 인공 신경망 모델에 층을 2개 추가해 보겠다. 여기서 만들 모델의 대략적인 구조는 다음 그림과 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4K9uvgxwFdL"
   },
   "source": [
    "- 첫 번째 이미지를 펼친 배열 ( 1 * 784 ) 를 입력층 784개의 뉴런에 넣는다. \n",
    "- 가운데에 100개의 뉴런을 가진 은닉층이 있다. \n",
    "- 마지막에 출력층으로 10개의 뉴런이 있고, \n",
    "- 각각의 출력층 값에 대해 소프트맥스 함수를 거쳐 최종 출력이 나온다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfaAjrUzwerI"
   },
   "source": [
    "1절을 만든 신경망 모델과 다른 점은 입력층과 출력층 사이에 밀집층이 추가된 것 <br>\n",
    "이렇게 입력층과 출력층 사이에 있는 모든 층을 ***은닉층***이라 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohLBB85ywp2I"
   },
   "source": [
    "은닉층 끝에는 활성화 함수가 있다. <br> \n",
    "***활성화 함수*** : 신경망 층의 선형 방정식의 계산 값에 적용하는 함수\n",
    "- 이전 절에서 출력층에 적용했던 소프트맥스 함수도 활성화 함수이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G91lfUnow6rJ"
   },
   "source": [
    "출력층에 적용하는 활성화 함수는 종류가 제한되어 있다. <br>\n",
    "- 이진 분류일 경우 시그모이드 함수\n",
    "- 다중 분류일 경우 소프트맥스 함수 <br>\n",
    "<-> 이에 비해 은닉층의 활성화 함수는 비교적 자유롭다. \n",
    "- 보통 시그모이드, ReLU 함수 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxT7URJLxLFo"
   },
   "source": [
    "### 왜 은닉층에 활성화 함수를 사용할까 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPUGJxqixODf"
   },
   "source": [
    "다음 2개의 선형 방정식을 보자. <br>\n",
    "``` \n",
    "a * 4 + 2 = b\n",
    "b * 3 - 5 = c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OENDPtJExWwH"
   },
   "source": [
    "왼쪽에서 첫 번째 식에 계산된 b가 두 번째 식에서 c를 계산하기 위해 쓰인다. <br>\n",
    "하지만 두 번째 식에 첫 번째 식을 대입하면 아래처럼 하나로 합쳐질 수 있다.  \n",
    "``` a * 12 + 1 = c ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7Vee2MAxr3I"
   },
   "source": [
    "신경망도 마찬가지이다. 은닉층에서 선형적인 산술 계산만 수행한다면 수행 역할이 없는 셈이다.  \n",
    "선형 계산을 적당하게 비선형적으로 비틀어 주어야 한다. 마치 다음과 같다.  \n",
    "```\n",
    "a * 4 + 2 = b \n",
    "log(b) = k\n",
    "k * 3 - 5 = c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZX-HEe2yBK4"
   },
   "source": [
    "많이 사용하는 활성화 함수 중 하나는 4장에서 배웠던 시그모이드 함수이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXqemnTwySSw"
   },
   "source": [
    "시그모이드 활성화 함수를 사용한 은닉층과 소프트맥스 함수를 사용한 출력층을  \n",
    "***케라스의 Dense 클래스***로 만들어 보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsrNIUr8ycMo"
   },
   "source": [
    "케라스에서 신경망의 첫 번째 층은 input_shape 매개변수로 입력의 크기를 꼭 정해 주어야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WrV7WbhQyhaH"
   },
   "outputs": [],
   "source": [
    "dense1 = keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,))\n",
    "dense2 = keras.layers.Dense(10, activation = 'softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2ZRSAu7ytbQ"
   },
   "source": [
    "dense1이 은닉층이고, 100개의 뉴런을 가진 밀집층이다.  \n",
    "활성화 함수를 'sigmoid'로 지정했고, input_shape 매개변수에서 입력의 크기를 (784,)로 지정했다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRKzxBtyy3LP"
   },
   "source": [
    "은닉층의 뉴런 개수를 지정하는 데 특별한 기준은 없다.  \n",
    "몇 개의 뉴런을 두어야 할 지 판단하기 위해서는 상당한 경험이 필요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZAVmnOAy-pn"
   },
   "source": [
    "한 가지 제약사항이 있다면 \n",
    "- 적어도 출력층의 뉴런보다는 많이 만들어야 한다. \n",
    "- 클래스 10개에 대한 확률을 예측해야 하는데 은닉층의 뉴런이 그보다 적다면  \n",
    "부족한 정보가 전달될 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QYQa8dIzIvH"
   },
   "source": [
    "그 다음 dense2는 출력층이다. 10개의 클래스를 분류하므로 10개의 뉴런을 두었고 활성화 함수는 소프트맥스 함수로 설정했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C872z4YJzQSX"
   },
   "source": [
    "## 심층 신경망 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nScb-8QRzSXP"
   },
   "source": [
    "이제 앞에서 만든 dense1과 dense2 객체를 Sequential 클래스에 추가하여 심층 신경망을 만들어 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BZ6NZHSrzYMY"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([dense1, dense2]) # 입력 데이터 자체가 입력층 하나를 담당 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1qfainUzeBn"
   },
   "source": [
    "Sequential 클래스의 객체를 만들 때 여러 개의 층을 추가하려면 이와 같이 dense1과 dense2를 리스트로 만들어 전달한다.  \n",
    "처음 등장하는 은닉층부터 출력층까지 순서에 맞게 리스트로 나열해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OVSRNWmAypF"
   },
   "source": [
    "인공신경망의 강력한 성능은 이렇게 층을 추가하여 입력 데이터에 대해 연속적인 학습을 진행하는 능력에서 나온다.  \n",
    "앞에서 나온 선형 회귀, 로지스틱 회귀, 결정 트리 등 다른 머신 러닝 알고리즘들과 대조된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AysqsVOSBG_N"
   },
   "source": [
    "케라스는 모델의 summary() 메서드를 호출하면 층에 대한 유용한 정보를 얻을 수가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN_7FUJRBKn9",
    "outputId": "faa75663-375b-484e-fd50-f2994a8cd550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z95BgnjBBMw8"
   },
   "source": [
    "- 맨 첫 줄 : 모델의 이름 \n",
    "- 이 모델에 들어있는 층이 순서대로 나열 \n",
    "- 층마다 층 이름, 클래스, 출력 크기, 모델 파라미터 개수가 출력된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccYyjREtBYVU"
   },
   "source": [
    "- 층을 만들 때 name 매개변수로 이름을 지정할 수 있다. 층 이름을 지정하지 않으면 자동으로 'dense'라고 이름을 붙인다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BQZLR21BewL"
   },
   "source": [
    "출력 크기를 보면 (None, 10)이다. 첫 번째 차원은 샘플의 개수를 나타낸다.  \n",
    "샘플 개수가 아직 정의되어 있지 않기 때문에 None이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX41cRBMBlNk"
   },
   "source": [
    "케라스 모델의 fit() 메서드에 훈련 데이터를 주입하면 이 데이터를 한 번에 모두 사용하지 않고,  \n",
    "잘게 나누어 여러 번에 걸쳐 경사 하강법 단계를 수행한다. 미니 배치 경사법을 수행하는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X4sicOvBxlN"
   },
   "source": [
    "두 번째 100은 쉽다. 은닉층의 뉴런 개수를 100개로 두었으니 100개의 출력이 나온다.  \n",
    "즉 샘플마다 784개의 픽셀값이 은닉층을 통과하며너 100개의 특성으로 압축되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlLuIO5kB8uc"
   },
   "source": [
    "마지막으로 모델 파라미터 개수가 출력된다.  \n",
    "이 층은 Dense 층이므로 입력 픽셀 784개와 100개의 모든 조합에 대한 가중치 + 각 입력마다 1개의 절편 ->  \n",
    "( 784 + 1 ) * 100 = 78500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl2U6GJnCI-U"
   },
   "source": [
    "두 번째 층의 출력 크기는 (None, 100)이다. 배치 차원은 동일하게 None이고 출력 뉴런 개수가 100개이기 때문  \n",
    "이 층의 모델 파라미터 개수는 몇 개일까 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cITQJwWuCRRk"
   },
   "source": [
    "100개의 은닉 층과 10개의 출력층 뉴런이 모두 연결되고 출력층의 뉴런마다 하나씩 절편이 있기 때문에  \n",
    "총 1,010개의 모델 파라미터 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOl3lM5eFBNU"
   },
   "source": [
    "마지막에는 총 모델 파라미터 개수와 훈련되는 파라미터 개수가 동일하게 79,510개로 나온다.  \n",
    "은닉층과 출력층의 파라미터를 합한 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AADm9BiEFJ9d"
   },
   "source": [
    "훈려되지 않는 파라미터(Non-trainable params)는 0으로 나온다. 간혹 경사 하강법으로 훈련되지 않는 파라미터가 나온다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZJtbRatFRdd"
   },
   "source": [
    "## 층을 추가하는 또다른 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LL6NKZ0PFTZc"
   },
   "source": [
    "모델을 훈련하기 전에 Sequential 클래스에 층을 추가하는 다른 방법이 있다.  \n",
    "앞에서는 Dense 클래스의 객체 Dense1과 Dense2를 만들어 Sequential 클래스에 전달했었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFyfab-o2u2J"
   },
   "source": [
    "이 두 객체를 따로 쓸 일이 없기 때문에 다음처럼 Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만드는 경우가 많다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TrGRPZOI29bx"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'),\n",
    "    keras.layers.Dense(10, activation='softmax', name='output')],\n",
    "    name='패션 MNIST 모델'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4qxBv4I3RdB"
   },
   "source": [
    "이렇게 작업하면 추가되는 층을 한눈에 쉽게 알아보는 장점이 있다.  \n",
    "이전과 달리 이번에는 Sequential 클래스의 name 매개변수로 모델의 이름을 지정했다.  \n",
    "또 Dense 층의 name 매개변수에 층의 이름을 'hidden', 'output'으로 각각 지정했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDGHlpG23eOh"
   },
   "source": [
    "모델의 이름과 달리 층의 이름은 반드시 영어여야 한다.  \n",
    "summary() 함수를 통해 모델의 이름이 잘 반영되었는지 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvXh1MqY3lQB",
    "outputId": "45d10978-431a-42c4-c95b-f554e78dc854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"패션 MNIST 모델\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden (Dense)              (None, 100)               78500     \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haT-XRoq3uFU"
   },
   "source": [
    "여러 모델과 많은 층을 사용할 때 name 매개변수를 사용하면 구분하기 쉽다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-JeFiwf3yoR"
   },
   "source": [
    "이 방법이 편리하지만, 아주 많은 층을 추가하려면 Sequential 클래스 생성자가 매우 길어진다.  \n",
    "또 조건에 따라 층을 추가할 수도 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knDZWbWX37IB"
   },
   "source": [
    "Sequential 클래스에서 층을 추가할 때 가장 널리 사용하는 방법은  \n",
    "모델의 add() 메서드이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6QUGm13i3--p"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential(name='FashionMNISTModel')\n",
    "model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGWqblHd4Owh"
   },
   "source": [
    "summary() 메서드의 결과에서 층과 파라미터 개수는 당연히 동일하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPE6tVXO4R94",
    "outputId": "5926d109-d507-484d-e434-5c7af5042a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH5e1Qg245d9"
   },
   "source": [
    "이제 모델을 훈련해 보자. compile() 메서드의 설정은 1절에서 했던 것과 동일하다.<br> 여기에서도 5번의 에포크 동안 훈련해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIiL1zSN5Ff5",
    "outputId": "57bed6e1-06eb-43dd-eb6c-35d05e438639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.5641 - accuracy: 0.8076\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.4080 - accuracy: 0.8529\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3731 - accuracy: 0.8657\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3507 - accuracy: 0.8725\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3342 - accuracy: 0.8797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf4d749280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_scaled, train_target, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4z-_Jk6AQHx"
   },
   "source": [
    "점수 : 0.9789 -- 시그모이드 함수를 사용했을 때와 비교하면 성능이 조금 우월해졌다.  \n",
    "그럼 검증 세트도 확인해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wL3epZAqAZjH",
    "outputId": "18f7b4a2-d419-473f-b9ff-208a4cd084b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3534 - accuracy: 0.8723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35339581966400146, 0.8723333477973938]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mu9zUe50Aden"
   },
   "source": [
    "1절의 은닉층을 추가하지 않은 경우보다 몇 퍼센트 성능이 향상되었다.  \n",
    "지금까지는 모델을 5번의 에포크 동안 훈련했다. 이보다 더 훈련을 하지 않을 이유가 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYbL52W2aPIc"
   },
   "source": [
    "## ReLU 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf5Yq3jcaRcT"
   },
   "source": [
    "시그모이드 그래프는 함수의 오른쪽과 왼쪽으로 갈 수록 그래프가 누워 있기 때문에  \n",
    "올바른 출력을 만드는 데 신속하게 대응하지 못한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpELL28VaZtT"
   },
   "source": [
    "층이 많은 심층 신경망일 수록 그 효과가 누적되어 학습을 더 어렵게 만든다.  \n",
    "이를 개선하기 위해 다른 종류의 활성화 함수가 제안되었다.  \n",
    "바로 **ReLU 함수**이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrDQkkZmajJT"
   },
   "source": [
    "렐루 함수는 \n",
    "``` f(x) = max(0, x)```이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6zj7bXCanu6"
   },
   "source": [
    "이미지 처리에서 좋은 성능을 낸다.  \n",
    "은닉층의 활성화 함수에서, 시그모이드 대신 ReLU를 적용하기 전에  \n",
    "케라스에서 제공하는 편리한 층 하나를 더 살펴보겠다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaNDJYQFa0eK"
   },
   "source": [
    "패션 MNIST 데이터는 28 * 28 크기이기 때문에 인공 신경망에 주입하기 위해  \n",
    "넘파이 배열의 reshape() 함수 메서드를 사용해 1차원으로 펼쳤다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPv839fVa85K"
   },
   "source": [
    "직접 이렇게 1차원으로 펼쳐도 좋지만, 케라스에서는 이를 위한 ***Flatten 층***을 제공한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CkDOpxubDLa"
   },
   "source": [
    "사실 Flatten 클래스는 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할만 한다.  \n",
    "입력에 곱해지는 가중치나 절편이 없다. 따라서 인공 신경망의 성능을 위해 기여하는 바는 없다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "883zvX4fbOKq"
   },
   "source": [
    "하지만 Flatten 클래스를 층처럼 입력층과 은닉층 사이에 추가하기 때문에 이를 층이라 부른다.  \n",
    "Flatten 층은 다음 코드처럼 입력층 바로 뒤에 추가한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2AkAWNaIbUQz"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = (28, 28)))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhDS3uh0bifS"
   },
   "source": [
    "첫 번째 Dense 층에 있던 input_shape 매개변수를 Flatten 층으로 옮겼다.  \n",
    "또 첫 번째 Dense 층의 activation 함수를 ReLU 함수로 변경하였다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftjxs3ySbrg6"
   },
   "source": [
    "하지만 이 신경망을 깊이가 3인 신경망이라 부르지 않는다.  \n",
    "Flatten 클래스는 학습하는 층이 아니기 때문이다.  \n",
    "모델의 summary() 함수 출력문을 보면 더 자세히 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiiOGB7Eb036",
    "outputId": "70e5c00b-6240-46d8-9976-de6c555f5add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5kghR8Hb2yq"
   },
   "source": [
    "첫 번째 등장하는 Flatten 층의 parameter는 0이다. 그냥 변환만 하고 전달하는 역할만 한다.  \n",
    "케라스의 Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있는 것이 장점이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sas2l-hucCoK"
   },
   "source": [
    "앞의 출력에서 784개의 입력이 첫 번째 은닉층에 전달된다. 이는 이전에 만들었던 모델에서는 눈치채기 어려웠다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRXsK5cEcHdS"
   },
   "source": [
    "입력 데이터에 대한 전처리 과정을 가능한 모델에 포함시키는 것이 케라스 API의 철학 중 하나이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0T8NkS4cNGy"
   },
   "source": [
    "그럼 훈련 데이터를 다시 준비해서 모델을 훈련해 보자.  \n",
    "reshape 함수를 사용하지 않았다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DJn_DJ7PcQyq"
   },
   "outputs": [],
   "source": [
    "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_scaled = train_input / 255.0\n",
    "\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(\n",
    "    train_scaled, train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcVJZ1J5c51C"
   },
   "source": [
    "모델을 컴파일하고 훈련하는 것은 다음 코드처럼 이전과 동일하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxZ1PHENdBhi",
    "outputId": "b2964814-9deb-4ad3-80ad-f2c47a3aec7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3543 - accuracy: 0.8731\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3318 - accuracy: 0.8811\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3194 - accuracy: 0.8863\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3050 - accuracy: 0.8925\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2970 - accuracy: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf4ad2f430>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_scaled, train_target, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afFQSdYrdN4q"
   },
   "source": [
    "시그모이드를 사용할 때보다 비교하면 성능이 조금 향상되었다.  \n",
    "크지 않지만 relu 함수를 조금 효과를 보았다.  \n",
    "그럼 검증 세트에서도 점수를 확인해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8kQk7T2dcXE",
    "outputId": "493e4085-1cd4-4357-a108-2642ab53b208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3775 - accuracy: 0.8768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3775230348110199, 0.8768333196640015]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW5npimpAu7P"
   },
   "source": [
    "## Optimizer\n",
    "그 전에 인공 신경망의 하이퍼파라미터에 대해 잠시 알아보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWFeb4oeA1wn"
   },
   "source": [
    "3장에서 하이퍼파라미터는 모델이 학습하지 않아 사람이 지정해 주어야 하는 파라미터이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjelyWNjA59H"
   },
   "source": [
    "이번 절에서는 은닉층을 하나 추가했다. 하지만 여러 개의 은닉층을 추가할 수도 있다.  \n",
    "***추가할 은닉층의 개수***는 우리가 정해줘야 할 하이퍼파라미터이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF7usK27BC2H"
   },
   "source": [
    "***각 은닉층의 뉴런 개수***도 하이퍼파라미터이다.  \n",
    "***활성화 함수의 종류***도 설정해야 할 하이퍼파라미터이다.  \n",
    "심지어 ***각 층의 종류***도 하이퍼파라미터이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAKByfwRBUCG"
   },
   "source": [
    "다른 종류의 층을 선택할 수 있다. ( 예 : 합성곱 층 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xiv2ZXOkBXhm"
   },
   "source": [
    "케라스는 기본적으로 미니배치 경사 하강법을 사용하며, 미니 배치 개수는 32개이다.  \n",
    "fit() 메서드의 batch_size 매개변수에서 이를 조정할 수 있으며 이 역시 하이퍼파라미터이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqjTTks8BhAe"
   },
   "source": [
    "epochs 매개변수도 하이퍼파라미터이다. 반복 횟수에 따라 다른 모델이 만들어진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDtqJdMlBlvf"
   },
   "source": [
    "마지막으로 compile() 메서드에서는 케라스의 기본 경사 하강법 알고리즘인 RMSprop을 사용한다.  \n",
    "케라스는 다양한 종류의 ***경사 하강법 알고리즘***을 제공한다.( = ***옵티마이저*** )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jv2kNSABPcG"
   },
   "source": [
    "RMSprop의 학습률 또한 조정할 하이퍼파라미터 중 하나이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raDnKJHoB4Sn"
   },
   "source": [
    "### SGD\n",
    "가장 기본적인 옵티마이저는 확률적 경사 하강법인 SGD이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIYv7cYCB8iA"
   },
   "source": [
    "이름이 SGD이지만 1개의 샘플을 뽑아서 훈련하지 않고, 앞서 언급한 것처럼 기본적으로 미니 배치를 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k5ShpWmCByG"
   },
   "source": [
    "SGD옵티마이저를 사용하려면 compile() 메서드의 optimizer를 'sgd'로 지정한다.  \n",
    "``` model.compile(optimizer = 'sgd', loss = 'sparse_categorical_crossentropy', \n",
    "      metrics = 'accuracy') ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOyL_Bl-COru"
   },
   "source": [
    "```\n",
    "sgd = keras.optimizers.SGD()\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntEvDUSlCcjf"
   },
   "source": [
    "만약 SGD 클래스의 학습률 기본값이 0.01일 때 이를 바꾸고 싶다면 다음과 같이 원하는 학습률을 learning_rate 매개변수에 지정하여 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxHNhajJClrX"
   },
   "source": [
    "```sgd.keras.optimizers.SGD(learning_rate = 0.1) ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBI60GReCsRS"
   },
   "source": [
    "기본 경사 하강법 옵티마이저는 모두 SGD 클래스에서 제공한다.  \n",
    "SGD 클래스의 momentum 매개변수의 기본값은 0이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DcYXkaOUkBt"
   },
   "source": [
    " 이를 0보다 큰 값으로 지정하면 마치 이전의 그래디언트를 가속도처럼 사용하는 ***모멘텀 최적화***를 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INch5FwzUviU"
   },
   "source": [
    "보통 momentum 매개변수는 0.9 이상을 지정한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHHhcCqnUzSz"
   },
   "source": [
    "다음처럼 SGD 클래스의 nesterov 매개변수를 기본값에서 True로 바꾸면 ***네스테로프 모멘텀 최적화***를 사용한다.  \n",
    "``` sgd = keras.optimizers.SGD(momentum = 0.9, nesterov = True) ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6kxB5vWU_wr"
   },
   "source": [
    "네스테로프 모멘텀은 모멘텀 최적화를 2번 반복하여 구현  \n",
    "대부분의 경우 네스테로프 모멘텀 최적화가 기본 확률적 경사 하강법보다 더 나은 성능 제공 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIB6cvG3VHf8"
   },
   "source": [
    "모델이 최적점에 가까이 갈 수록 학습률을 낮출 수 있다. 이렇게 하면 안정적으로 최적점에 수렴할 확률이 높다.  \n",
    "이런 학습률을 ***적응적 학습률***이라 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b5zR7qgVPrM"
   },
   "source": [
    "적응적 학습률을 사용하는 대표적인 옵티마이저는 ***Adagrad*** , ***RMSprop***이다.  \n",
    "``` adagrad = keras.optimizers.Adagard() ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c0gYIqoVdxb"
   },
   "source": [
    "모멘텀 최적화와 RMSprop의 장점을 접목한 것이 ***Adam***이다.  \n",
    "Adam, Adagrad, RMSprop 이 세 가지는 learning_rate 매개변수의 기본값으로 모두 0.001 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SmchjX8V2XM"
   },
   "source": [
    "### 옵티마이저 훈련 예시 : Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a328ccbaVv9T"
   },
   "source": [
    "여기에서는 Adam 클래스의 매개변수 기본값을 사용해 MNIST 모델을 훈련해 보았다.  \n",
    "먼저 모델을 다시 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "aksMMIHfV7Lr"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfBksq9jWROT"
   },
   "source": [
    "compile() 메서드의 optimizer를 'adam'으로 설정하고 5번 동안 훈련한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJcyRK4iWWQj",
    "outputId": "95dc9dd1-79c9-4066-a262-810688f946fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.5246 - accuracy: 0.8161\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3922 - accuracy: 0.8577\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3493 - accuracy: 0.8737\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3260 - accuracy: 0.8817\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.3059 - accuracy: 0.8887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf4ab62df0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "model.fit(train_scaled, train_target, epochs=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0e7-ifgdr16"
   },
   "source": [
    "이 출력 결과를 보면 기본 RMSprop을 사용했을 때와 거의 같은 성능을 보여준다.  \n",
    "마지막으로 검증 세트에서의 성능도 확인해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WN4KW1hd0W6",
    "outputId": "b30b8f77-a122-465f-b99a-6cdff60dbd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3363 - accuracy: 0.8785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3362897038459778, 0.8784999847412109]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
